{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a5d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "from epilearn.data import UniversalDataset\n",
    "from epilearn.utils import transforms\n",
    "from epilearn.tasks.forecast import Forecast\n",
    "from epilearn.tasks.detection import Detection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff3a95",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa35c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from epilearn.models.Temporal.base import BaseModel\n",
    "from epilearn.models.Spatial.base import BaseModel\n",
    "from epilearn.models.SpatialTemporal.base import BaseModel\n",
    "\n",
    "class CustomizedTemporal(BaseModel):\n",
    "    def __init__(self,\n",
    "                num_features,\n",
    "                num_timesteps_input,\n",
    "                num_timesteps_output,\n",
    "                hidden_size,\n",
    "                num_layers,\n",
    "                bidirectional,\n",
    "                device = 'cpu'):\n",
    "        super(CustomizedTemporal, self).__init__(device=device)\n",
    "        self.num_feats = num_features\n",
    "        self.hidden = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional=bidirectional\n",
    "        self.lookback = num_timesteps_input\n",
    "        self.horizon = num_timesteps_output\n",
    "        self.device = device\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.num_feats, hidden_size=self.hidden, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional)\n",
    "        self.fc = nn.Linear(self.hidden, self.horizon)\n",
    "\n",
    "    def forward(self, feature, graph=None, states=None, dynamic_graph=None, **kargs):        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(feature)  # out: tensor of shape (batch, seq_length, hidden_size * num_directions)\n",
    "        \n",
    "        # Decode the last hidden state\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "    def initialize(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "\n",
    "class CustomizedSpatial(BaseModel):\n",
    "    def __init__(self,\n",
    "                num_nodes,\n",
    "                num_features,\n",
    "                num_timesteps_input,\n",
    "                num_timesteps_output,\n",
    "                hidden_size,\n",
    "                device = 'cpu'):\n",
    "        super(CustomizedSpatial, self).__init__(device=device)\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_feats = num_features\n",
    "        self.hidden = hidden_size\n",
    "        self.lookback = num_timesteps_input\n",
    "        self.horizon = num_timesteps_output\n",
    "        self.device = device\n",
    "\n",
    "        self.gcn = GCNConv(in_channels=self.num_feats, out_channels=self.hidden)\n",
    "        self.fc = nn.Linear(self.hidden, self.horizon)\n",
    "\n",
    "    def forward(self, feature, graph, states=None, dynamic_graph=None, **kargs):\n",
    "        x = feature.transpose(1,2).reshape(-1, self.num_nodes, self.num_feats)\n",
    "        edge_index, _ = dense_to_sparse(graph)\n",
    "\n",
    "        x = self.gcn(x, edge_index=edge_index)\n",
    "\n",
    "        \n",
    "        out = self.fc(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def initialize(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class CustomizedSpatialTemporal(BaseModel):\n",
    "    def __init__(self,\n",
    "                num_nodes,\n",
    "                num_features,\n",
    "                num_timesteps_input,\n",
    "                num_timesteps_output,\n",
    "                num_channels_output,\n",
    "                hidden_size,\n",
    "                num_layers,\n",
    "                bidirectional,\n",
    "                device = 'cpu'):\n",
    "        super(CustomizedSpatialTemporal, self).__init__(device=device)\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_feats = num_features\n",
    "        self.hidden = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional=bidirectional\n",
    "        self.lookback = num_timesteps_input\n",
    "        self.horizon = num_timesteps_output\n",
    "        self.num_out_channels = num_channels_output\n",
    "        self.device = device\n",
    "\n",
    "        self.gcn = GCNConv(in_channels=self.num_feats, out_channels=self.hidden)\n",
    "        self.lstm = nn.LSTM(input_size=self.hidden, hidden_size=self.hidden, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional)\n",
    "        self.fc = nn.Linear(self.hidden, self.num_out_channels*self.horizon)\n",
    "\n",
    "    def forward(self, feature, graph, states=None, dynamic_graph=None, **kargs):\n",
    "        # message passing to update node features\n",
    "        edge_index, _ = dense_to_sparse(graph)\n",
    "\n",
    "        x = self.gcn(feature.float(), edge_index=edge_index)\n",
    "\n",
    "        x = x.transpose(1,2).reshape(-1, self.lookback, self.hidden)\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x)  # out: tensor of shape (batch, seq_length, hidden_size * num_directions)\n",
    "        # Decode the last hidden state\n",
    "        out = out[:, -1, :]\n",
    "        out = out.reshape(-1, self.num_nodes, self.hidden)\n",
    "        out = self.fc(out).reshape(-1, self.num_nodes, self.horizon, self.num_out_channels)\n",
    "\n",
    "        return out.transpose(1,2) # return shape (batch, horizon, num_nodes, num_channels_output)\n",
    "\n",
    "    def initialize(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd42e0a1",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8bdc1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
