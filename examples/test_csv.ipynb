{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EpiLearn Dataset Loading Test\n",
        "\n",
        "This Notebook is used to test the data loading functionality of the EpiLearn framework, including the following two test methods:\n",
        "\n",
        "1. **CSV Dataset Test** - Test loading custom data from CSV files\n",
        "2. **Toy Dataset Test** - Test built-in toy datasets\n",
        "\n",
        "## Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/local/scratch3/yli3466/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Successfully imported EpiLearn modules\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "# Add EpiLearn to Python path\n",
        "current_dir = os.path.dirname(os.path.abspath('.'))\n",
        "sys.path.append(current_dir)\n",
        "\n",
        "try:\n",
        "    from epilearn.data import UniversalDataset\n",
        "    from epilearn.utils import transforms\n",
        "    print(\"✓ Successfully imported EpiLearn modules\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ Import failed: {e}\")\n",
        "    print(\"Please ensure EpiLearn is properly installed\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions\n",
        "def print_separator(title):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"  {title}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "def print_tensor_info(name, tensor):\n",
        "    if tensor is not None:\n",
        "        print(f\"  {name:15}: {tensor.shape} | {tensor.dtype} | Memory: {tensor.numel() * tensor.element_size() / 1024:.1f} KB\")\n",
        "        if tensor.numel() > 0:\n",
        "            print(f\"  {'':<15}  Range: [{tensor.min().item():.4f}, {tensor.max().item():.4f}]\")\n",
        "    else:\n",
        "        print(f\"  {name:15}: None\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. CSV Dataset Test\n",
        "\n",
        "This test demonstrates how to load custom data from CSV files, including automatic feature column and target column detection functionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def infer_cols(csv_path, node_col='node', time_col='time',\n",
        "                target_hints=('y','target','label'), target_regex=None, sample_rows=None,\n",
        "                manual_feature_cols=None, manual_target_cols=None):\n",
        "    \"\"\"\n",
        "    Automatically infer feature_cols / target_cols from CSV, with manual override options.\n",
        "    \n",
        "    Parameters:\n",
        "    - target_hints: Common target column names (case insensitive)\n",
        "    - target_regex: Optional regex pattern (e.g., r'^y(_.*)?$')\n",
        "    - sample_rows: Only read first N rows for quick inference; None for full table\n",
        "    - manual_feature_cols: Manually specified feature column list, overrides auto inference if provided\n",
        "    - manual_target_cols: Manually specified target column list, overrides auto inference if provided\n",
        "    \n",
        "    Returns:\n",
        "    - feature_cols: Final feature columns to use\n",
        "    - target_cols: Final target columns to use\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path, nrows=sample_rows)\n",
        "    cols = df.columns.tolist()\n",
        "    if node_col not in cols or time_col not in cols:\n",
        "        raise ValueError(f\"Missing key columns: node_col='{node_col}', time_col='{time_col}'\")\n",
        "\n",
        "    # Candidate columns = all columns except node/time\n",
        "    user_cols = [c for c in cols if c not in (node_col, time_col)]\n",
        "\n",
        "    # Keep only \"numeric-like\" columns (at least one value can be converted to numeric)\n",
        "    numeric_candidates = []\n",
        "    for c in user_cols:\n",
        "        s = pd.to_numeric(df[c], errors='coerce')\n",
        "        if (~s.isna()).any():\n",
        "            numeric_candidates.append(c)\n",
        "\n",
        "    # Auto-infer target columns\n",
        "    lowers = {c: c.lower() for c in numeric_candidates}\n",
        "    t_candidates = [c for c, lc in lowers.items()\n",
        "                    if lc in target_hints or lc.startswith('y_')]\n",
        "    if target_regex:\n",
        "        import re\n",
        "        pat = re.compile(target_regex)\n",
        "        t_candidates += [c for c in numeric_candidates if pat.fullmatch(c)]\n",
        "\n",
        "    # Remove duplicates while preserving original order\n",
        "    seen = set()\n",
        "    auto_target_cols = [c for c in t_candidates if not (c in seen or seen.add(c))]\n",
        "    auto_target_cols = auto_target_cols or None\n",
        "\n",
        "    # Auto-infer feature columns = numeric candidates - target columns\n",
        "    auto_feature_cols = [c for c in numeric_candidates if c not in set(auto_target_cols or [])]\n",
        "\n",
        "    # Decide final columns: manual > auto\n",
        "    final_target_cols = manual_target_cols if manual_target_cols is not None else auto_target_cols\n",
        "    final_feature_cols = manual_feature_cols if manual_feature_cols is not None else auto_feature_cols\n",
        "\n",
        "    # Print results\n",
        "    print(\"🔎 [Column Selection Results]\")\n",
        "    print(f\"  All columns: {cols}\")\n",
        "    print(f\"  Numeric candidates: {numeric_candidates}\")\n",
        "    print(f\"  Auto-inferred target_cols: {auto_target_cols}\")\n",
        "    print(f\"  Auto-inferred feature_cols: {auto_feature_cols}\")\n",
        "    \n",
        "    if manual_target_cols is not None:\n",
        "        print(f\"  🎯 Manually specified target_cols: {manual_target_cols}\")\n",
        "    if manual_feature_cols is not None:\n",
        "        print(f\"  🎯 Manually specified feature_cols: {manual_feature_cols}\")\n",
        "        \n",
        "    print(f\"  ✅ Final target_cols: {final_target_cols}\")\n",
        "    print(f\"  ✅ Final feature_cols: {final_feature_cols}\")\n",
        "\n",
        "    return final_feature_cols, final_target_cols\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Demo 1: Auto-infer columns\n",
            "\n",
            "============================================================\n",
            "  Test CSV Dataset (Manual/Auto Column Selection)\n",
            "============================================================\n",
            "🔎 [Column Selection Results]\n",
            "  All columns: ['time', 'node', 'f0', 'f1', 'f2', 'f3', 'y']\n",
            "  Numeric candidates: ['f0', 'f1', 'f2', 'f3', 'y']\n",
            "  Auto-inferred target_cols: ['y']\n",
            "  Auto-inferred feature_cols: ['f0', 'f1', 'f2', 'f3']\n",
            "  ✅ Final target_cols: ['y']\n",
            "  ✅ Final feature_cols: ['f0', 'f1', 'f2', 'f3']\n",
            "\n",
            "✅ CSV loaded:\n",
            "  x: torch.Size([539, 47, 4]) | y: torch.Size([539, 47])\n",
            "  graph: torch.Size([47, 47]) | edge_index: torch.Size([2, 2189])\n",
            "\n",
            "📊 CSV Data Dimension Info:\n",
            "  x (features)   : torch.Size([539, 47, 4]) | torch.float32 | Memory: 395.8 KB\n",
            "                   Range: [-0.6547, 5908.0000]\n",
            "  y (targets)    : torch.Size([539, 47]) | torch.float32 | Memory: 99.0 KB\n",
            "                   Range: [0.0000, 5908.0000]\n",
            "  graph (static) : torch.Size([47, 47]) | torch.float32 | Memory: 8.6 KB\n",
            "                   Range: [0.0000, 1.0000]\n",
            "  edge_index     : torch.Size([2, 2189]) | torch.int64 | Memory: 34.2 KB\n",
            "                   Range: [0.0000, 46.0000]\n",
            "✅ CSV data loading test passed!\n"
          ]
        }
      ],
      "source": [
        "def test_csv_dataset(manual_feature_cols=None, manual_target_cols=None):\n",
        "    \"\"\"\n",
        "    CSV Test: Read existing CSV files -> from_csv loading -> print data info\n",
        "    \n",
        "    Parameters:\n",
        "    - manual_feature_cols: Manually specified feature columns, e.g., ['f1', 'f2']\n",
        "    - manual_target_cols: Manually specified target columns, e.g., ['y']\n",
        "    \n",
        "    If manual columns are not provided, auto-inferred results will be used\n",
        "    \"\"\"\n",
        "    print_separator(\"Test CSV Dataset (Manual/Auto Column Selection)\")\n",
        "\n",
        "    try:\n",
        "        os.makedirs('./datasets', exist_ok=True)\n",
        "        feat_path = 'toy_features.csv'\n",
        "        edge_path = '/toy_edges.csv'\n",
        "\n",
        "        # Smart column selection: support manual override of auto inference\n",
        "        feature_cols, target_cols = infer_cols(\n",
        "            feat_path,\n",
        "            node_col='node',\n",
        "            time_col='time',\n",
        "            target_hints=('y','target','label'),\n",
        "            target_regex=None,\n",
        "            sample_rows=None,\n",
        "            manual_feature_cols=manual_feature_cols,  # New: manual feature columns\n",
        "            manual_target_cols=manual_target_cols     # New: manual target columns\n",
        "        )\n",
        "\n",
        "        # Load data using finalized columns\n",
        "        ds = UniversalDataset.from_csv(\n",
        "            feature_csv=feat_path,\n",
        "            node_id_col=\"node\",\n",
        "            time_col=\"time\",\n",
        "            feature_cols=feature_cols,\n",
        "            target_cols=target_cols,\n",
        "            edge_csv=edge_path\n",
        "        )\n",
        "\n",
        "        # Simple print verification\n",
        "        print(\"\\n✅ CSV loaded:\")\n",
        "        print(\"  x:\", ds.x.shape, \"| y:\", None if ds.y is None else ds.y.shape)\n",
        "        print(\"  graph:\", None if ds.graph is None else ds.graph.shape,\n",
        "              \"| edge_index:\", None if ds.edge_index is None else ds.edge_index.shape)\n",
        "\n",
        "        # Print basic information\n",
        "        print(\"\\n📊 CSV Data Dimension Info:\")\n",
        "        print_tensor_info(\"x (features)\", ds.x)\n",
        "        print_tensor_info(\"y (targets)\", ds.y)\n",
        "        print_tensor_info(\"graph (static)\", ds.graph)\n",
        "        print_tensor_info(\"edge_index\", ds.edge_index)\n",
        "\n",
        "        print(\"✅ CSV data loading test passed!\")\n",
        "        return ds\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ CSV dataset test failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Demo 1: Use auto inference (default behavior)\n",
        "print(\"🔄 Demo 1: Auto-infer columns\")\n",
        "csv_dataset_auto = test_csv_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔄 Demo 2: Manual column specification\n",
            "\n",
            "============================================================\n",
            "  Test CSV Dataset (Manual/Auto Column Selection)\n",
            "============================================================\n",
            "🔎 [Column Selection Results]\n",
            "  All columns: ['time', 'node', 'f0', 'f1', 'f2', 'f3', 'y']\n",
            "  Numeric candidates: ['f0', 'f1', 'f2', 'f3', 'y']\n",
            "  Auto-inferred target_cols: ['y']\n",
            "  Auto-inferred feature_cols: ['f0', 'f1', 'f2', 'f3']\n",
            "  🎯 Manually specified target_cols: ['y']\n",
            "  🎯 Manually specified feature_cols: ['f1', 'f2']\n",
            "  ✅ Final target_cols: ['y']\n",
            "  ✅ Final feature_cols: ['f1', 'f2']\n",
            "\n",
            "✅ CSV loaded:\n",
            "  x: torch.Size([539, 47, 2]) | y: torch.Size([539, 47])\n",
            "  graph: torch.Size([47, 47]) | edge_index: torch.Size([2, 2189])\n",
            "\n",
            "📊 CSV Data Dimension Info:\n",
            "  x (features)   : torch.Size([539, 47, 2]) | torch.float32 | Memory: 197.9 KB\n",
            "                   Range: [-0.6547, 6.0000]\n",
            "  y (targets)    : torch.Size([539, 47]) | torch.float32 | Memory: 99.0 KB\n",
            "                   Range: [0.0000, 5908.0000]\n",
            "  graph (static) : torch.Size([47, 47]) | torch.float32 | Memory: 8.6 KB\n",
            "                   Range: [0.0000, 1.0000]\n",
            "  edge_index     : torch.Size([2, 2189]) | torch.int64 | Memory: 34.2 KB\n",
            "                   Range: [0.0000, 46.0000]\n",
            "✅ CSV data loading test passed!\n"
          ]
        }
      ],
      "source": [
        "# Demo 2: Manual column specification (override auto inference)\n",
        "print(\"\\n🔄 Demo 2: Manual column specification\")\n",
        "csv_dataset_manual = test_csv_dataset(\n",
        "    manual_feature_cols=['f1', 'f2'],  # Manually specify feature columns\n",
        "    manual_target_cols=['y']           # Manually specify target columns\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Toy Dataset Test\n",
        "\n",
        "This test demonstrates how to load EpiLearn's built-in toy dataset for quick framework functionality verification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "  Test Toy Dataset\n",
            "============================================================\n",
            "📦 Starting to download toy dataset...\n",
            "✓ Toy dataset loaded successfully!\n",
            "\n",
            "📊 Data Dimension Info:\n",
            "  x (features)   : torch.Size([539, 47, 4]) | torch.float32 | Memory: 395.8 KB\n",
            "                   Range: [-0.6547, 5908.0000]\n",
            "  y (targets)    : torch.Size([539, 47]) | torch.float32 | Memory: 99.0 KB\n",
            "                   Range: [0.0000, 5908.0000]\n",
            "  states         : torch.Size([539, 47, 3]) | torch.float32 | Memory: 296.9 KB\n",
            "                   Range: [0.0000, 14047001.0000]\n",
            "  graph (static) : torch.Size([47, 47]) | torch.float32 | Memory: 8.6 KB\n",
            "                   Range: [0.0000, 5424283.0000]\n",
            "  dynamic_graph  : torch.Size([539, 47, 47, 1]) | torch.float32 | Memory: 4651.0 KB\n",
            "                   Range: [0.0000, 969939.0000]\n",
            "  edge_index     : torch.Size([2, 2189]) | torch.int64 | Memory: 34.2 KB\n",
            "                   Range: [0.0000, 46.0000]\n",
            "  edge_weight    : torch.Size([2189]) | torch.float32 | Memory: 8.6 KB\n",
            "                   Range: [1.0000, 5424283.0000]\n",
            "\n",
            "📏 Dataset length: 539\n",
            "\n",
            "🔍 First sample:\n",
            "  Type: <class 'dict'>\n"
          ]
        }
      ],
      "source": [
        "def test_toy_dataset():\n",
        "    print_separator(\"Test Toy Dataset\")\n",
        "    try:\n",
        "        dataset = UniversalDataset()\n",
        "        print(\"📦 Starting to download toy dataset...\")\n",
        "        dataset.load_toy_dataset()\n",
        "        print(\"✓ Toy dataset loaded successfully!\")\n",
        "\n",
        "        print(\"\\n📊 Data Dimension Info:\")\n",
        "        print_tensor_info(\"x (features)\", dataset.x)\n",
        "        print_tensor_info(\"y (targets)\", dataset.y)\n",
        "        print_tensor_info(\"states\", dataset.states)\n",
        "        print_tensor_info(\"graph (static)\", dataset.graph)\n",
        "        print_tensor_info(\"dynamic_graph\", dataset.dynamic_graph)\n",
        "        print_tensor_info(\"edge_index\", dataset.edge_index)\n",
        "        print_tensor_info(\"edge_weight\", dataset.edge_weight)\n",
        "\n",
        "        print(f\"\\n📏 Dataset length: {len(dataset)}\")\n",
        "\n",
        "        if len(dataset) > 0:\n",
        "            sample = dataset[0]\n",
        "            print(f\"\\n🔍 First sample:\")\n",
        "            print(f\"  Type: {type(sample)}\")\n",
        "            if hasattr(sample, 'x'):\n",
        "                print(f\"  sample.x: {sample.x.shape if sample.x is not None else 'None'}\")\n",
        "            if hasattr(sample, 'y'):\n",
        "                print(f\"  sample.y: {sample.y.shape if sample.y is not None else 'None'}\")\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Toy dataset test failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Execute toy dataset test\n",
        "toy_dataset = test_toy_dataset()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Summary\n",
        "\n",
        "The following summarizes test results and displays basic dataset information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "  Test Summary\n",
            "============================================================\n",
            "✅ Successfully loaded datasets: 3\n",
            "  - CSV Auto-inference\n",
            "  - CSV Manual Specification\n",
            "  - Toy Dataset\n",
            "\n",
            "⏰ Test completion time: 2025-08-26 12:45:06\n",
            "🎉 Dataset testing completed! You can now start using EpiLearn for epidemiological modeling.\n",
            "\n",
            "📝 Usage Instructions:\n",
            "  1. Auto-inference mode: test_csv_dataset() - automatically detect feature and target columns\n",
            "  2. Manual specification mode: test_csv_dataset(manual_feature_cols=['col1', 'col2'], manual_target_cols=['target'])\n",
            "  3. Hybrid mode: manually specify only one type, use auto-inference for the other\n"
          ]
        }
      ],
      "source": [
        "# Summarize test results\n",
        "print_separator(\"Test Summary\")\n",
        "\n",
        "datasets = []\n",
        "if 'csv_dataset_auto' in locals() and csv_dataset_auto is not None:\n",
        "    datasets.append((\"CSV Auto-inference\", csv_dataset_auto))\n",
        "if 'csv_dataset_manual' in locals() and csv_dataset_manual is not None:\n",
        "    datasets.append((\"CSV Manual Specification\", csv_dataset_manual))\n",
        "if 'toy_dataset' in locals() and toy_dataset is not None:\n",
        "    datasets.append((\"Toy Dataset\", toy_dataset))\n",
        "\n",
        "print(f\"✅ Successfully loaded datasets: {len(datasets)}\")\n",
        "for name, dataset in datasets:\n",
        "    print(f\"  - {name}\")\n",
        "    \n",
        "if not datasets:\n",
        "    print(\"❌ No datasets loaded successfully\")\n",
        "else:\n",
        "    print(f\"\\n⏰ Test completion time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(\"🎉 Dataset testing completed! You can now start using EpiLearn for epidemiological modeling.\")\n",
        "    \n",
        "print(\"\\n📝 Usage Instructions:\")\n",
        "print(\"  1. Auto-inference mode: test_csv_dataset() - automatically detect feature and target columns\")\n",
        "print(\"  2. Manual specification mode: test_csv_dataset(manual_feature_cols=['col1', 'col2'], manual_target_cols=['target'])\")\n",
        "print(\"  3. Hybrid mode: manually specify only one type, use auto-inference for the other\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vllm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
